# ðŸŒ€ Mini Diffusion Model for Text-to-MNIST Generation

A lightweight diffusion model that generates **MNIST digits** (0â€“9) from **text prompts** like `"zero"`, `"one"`, etc.  
Built with **PyTorch**, optimized for **Google Colab (CPU/GPU T4)**.

---

## ðŸš€ Overview

This project implements a **Mini Diffusion Model** (DDPM-style) that learns to generate handwritten MNIST digits conditioned on text labels.  
Itâ€™s designed for simplicity, clarity, and full trainability within **10â€“15 minutes on a Colab T4 GPU**.

---

## âœ¨ Key Features

### ðŸ“Š Architecture
- **Mini U-Net** with encoderâ€“decoder structure  
- **Sinusoidal position embeddings** for timestep encoding  
- **Text conditioning** via one-hot vectors (digits 0â€“9)  
- **Skip connections** for stable training and better gradient flow  

### ðŸ”§ Training
- **Forward Diffusion:** Linear noise schedule (Î² from `1e-4` â†’ `0.02`)  
- **1000 diffusion timesteps**  
- **Loss:** Mean Squared Error (MSE) between predicted and true noise  
- **Optimizer:** Adam  
- **Fully Colab-optimized** (runs on CPU or T4 GPU)

### ðŸŽ¯ Sampling
- **Reverse diffusion** using DDPM formulation  
- Generate digits directly from text prompts like `"seven"` or `"three"`  
- **Iterative denoising** across 1000 steps for each sample  

---

## ðŸ§  What It Does
âœ… Loads and preprocesses MNIST dataset  
âœ… Creates mappings between text labels and digit indices  
âœ… Implements forward diffusion (adds noise progressively)  
âœ… Trains U-Net to predict the added noise  
âœ… Uses reverse diffusion to generate clean samples  
âœ… Visualizes generated digits and training loss curves  
âœ… Saves the trained model  

---

## ðŸ’» Usage (in Google Colab)

```python
# Clone or upload your script to Colab and run all cells.
```
---
(kid like notebook just run it .. its too easy)
