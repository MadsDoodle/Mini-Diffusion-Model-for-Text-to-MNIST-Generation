{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "828c6c06b35e438f8715d5e7daa1d5a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9984bf2b2564a5cbcc2852bcc77a403",
              "IPY_MODEL_651163a3e8e4444da63183877e7d42b2",
              "IPY_MODEL_2168f7c1c5b544fca0fa20822241f617"
            ],
            "layout": "IPY_MODEL_14a9545524304b78ab7d086ddadaedbc"
          }
        },
        "e9984bf2b2564a5cbcc2852bcc77a403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faefabf89cf547cdbd0d044543fd59d7",
            "placeholder": "​",
            "style": "IPY_MODEL_d7dd82379abd45839ea27332432b6369",
            "value": "Epoch 1/10: 100%"
          }
        },
        "651163a3e8e4444da63183877e7d42b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b53acde324143b4ac01e4140e4d9887",
            "max": 209,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6df901b7cbf04d039176bff474bc1a0a",
            "value": 209
          }
        },
        "2168f7c1c5b544fca0fa20822241f617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6db269e192d249a5a175f79b4a7b45d1",
            "placeholder": "​",
            "style": "IPY_MODEL_6993a97f02bb4bd4babf9e7401c471b7",
            "value": " 209/209 [14:29&lt;00:00,  3.23s/it, loss=0.0036, lr=2.50e-09]"
          }
        },
        "14a9545524304b78ab7d086ddadaedbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faefabf89cf547cdbd0d044543fd59d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7dd82379abd45839ea27332432b6369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b53acde324143b4ac01e4140e4d9887": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6df901b7cbf04d039176bff474bc1a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6db269e192d249a5a175f79b4a7b45d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6993a97f02bb4bd4babf9e7401c471b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5800b2b9957d488ba8a89c7760e14433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b80a22ac036460f9c320ba5dbeaeeff",
              "IPY_MODEL_d8eac01d479e4e689a19e8a1f3d4b67c",
              "IPY_MODEL_e955cb6b90dd4e3fbcc24eeea0717a33"
            ],
            "layout": "IPY_MODEL_3e8d8bbc75f243cd9ca08320b00c05cf"
          }
        },
        "2b80a22ac036460f9c320ba5dbeaeeff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3e6c3b0d26b4e49a6171a77faadff82",
            "placeholder": "​",
            "style": "IPY_MODEL_49dee07e1c5b45cc8d664166f1fa263c",
            "value": "Epoch 2/10: 100%"
          }
        },
        "d8eac01d479e4e689a19e8a1f3d4b67c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6917c1bc6aa4cc0a81a3833eb120933",
            "max": 209,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f0cac18e7b04d3da76ae10e03715772",
            "value": 209
          }
        },
        "e955cb6b90dd4e3fbcc24eeea0717a33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d7ac9ffa2b549d98f1cf1da41600942",
            "placeholder": "​",
            "style": "IPY_MODEL_75582ded990946849816ed84c24d1846",
            "value": " 209/209 [14:29&lt;00:00,  3.24s/it, loss=0.0366, lr=2.50e-09]"
          }
        },
        "3e8d8bbc75f243cd9ca08320b00c05cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3e6c3b0d26b4e49a6171a77faadff82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49dee07e1c5b45cc8d664166f1fa263c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6917c1bc6aa4cc0a81a3833eb120933": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f0cac18e7b04d3da76ae10e03715772": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d7ac9ffa2b549d98f1cf1da41600942": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75582ded990946849816ed84c24d1846": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0322a4b9ec904944b23c29b9b025346a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f02ae30a87bf4c6b9f9d6e53e8eb0cb4",
              "IPY_MODEL_5bd1eb08c1a74847a77b42da489893da",
              "IPY_MODEL_43d894059f4f478a92102cfbc73d6ebc"
            ],
            "layout": "IPY_MODEL_8ca0626871ad435893db951c15acee9d"
          }
        },
        "f02ae30a87bf4c6b9f9d6e53e8eb0cb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e985e83fedb47f2af7b9bc3bedd2350",
            "placeholder": "​",
            "style": "IPY_MODEL_989495a6d4004616abb6c0704ed7a983",
            "value": "Epoch 3/10: 100%"
          }
        },
        "5bd1eb08c1a74847a77b42da489893da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0508799203d54ffeac07c8b1a57e603a",
            "max": 209,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c50e777888049879c8a13317f06ff1f",
            "value": 209
          }
        },
        "43d894059f4f478a92102cfbc73d6ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18aa52964d75451bad512d51afab2d38",
            "placeholder": "​",
            "style": "IPY_MODEL_ee5f24da197647c49272f0bc18e54b82",
            "value": " 209/209 [14:29&lt;00:00,  3.23s/it, loss=0.1458, lr=2.50e-09]"
          }
        },
        "8ca0626871ad435893db951c15acee9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e985e83fedb47f2af7b9bc3bedd2350": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "989495a6d4004616abb6c0704ed7a983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0508799203d54ffeac07c8b1a57e603a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c50e777888049879c8a13317f06ff1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18aa52964d75451bad512d51afab2d38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee5f24da197647c49272f0bc18e54b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9eac30a469c041ca9cbda5d3be0901fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6445b742360e44b4888c645ced902591",
              "IPY_MODEL_fd5d1da8295b495c93c439d07451ee8d",
              "IPY_MODEL_d9a38eaa8d1448b2ae0e8447b855de47"
            ],
            "layout": "IPY_MODEL_cdb329d5ec684f7eb2a674af1f7bc471"
          }
        },
        "6445b742360e44b4888c645ced902591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba2f59f2cf544edea12da493d5194c77",
            "placeholder": "​",
            "style": "IPY_MODEL_0641c24dc1384bddb97aa1f781d5b438",
            "value": "Epoch 4/10:  44%"
          }
        },
        "fd5d1da8295b495c93c439d07451ee8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a6645b526fd434187d5e05d066919e5",
            "max": 209,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3593d416275f4f34af138e1a67125931",
            "value": 91
          }
        },
        "d9a38eaa8d1448b2ae0e8447b855de47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f5829be730c42c084fe126b3fe944fc",
            "placeholder": "​",
            "style": "IPY_MODEL_39807bd944594c3a86bcecb02e436a44",
            "value": " 91/209 [06:19&lt;08:11,  4.17s/it, loss=0.0534, lr=2.50e-09]"
          }
        },
        "cdb329d5ec684f7eb2a674af1f7bc471": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba2f59f2cf544edea12da493d5194c77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0641c24dc1384bddb97aa1f781d5b438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a6645b526fd434187d5e05d066919e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3593d416275f4f34af138e1a67125931": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f5829be730c42c084fe126b3fe944fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39807bd944594c3a86bcecb02e436a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "754de80131de42a7a6d6cf6c34e2de9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_abc70629c2ac407882ad3a754c2cc97d",
              "IPY_MODEL_4e7a8aeef944416ab7feecb5aab378e5",
              "IPY_MODEL_5e71c9744afb4f0cafb5fddd8649696c"
            ],
            "layout": "IPY_MODEL_a662d32b2f904f9d9b4fe648af2399ba"
          }
        },
        "abc70629c2ac407882ad3a754c2cc97d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e4ba0f00cd347a18385c0bfba48a20c",
            "placeholder": "​",
            "style": "IPY_MODEL_36040609f6e54f2788b309dff18a94a0",
            "value": "Epoch 1/10:   1%"
          }
        },
        "4e7a8aeef944416ab7feecb5aab378e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38271774715e4747af6211925791a283",
            "max": 209,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bafa90c3b59458dae6bc01d8d7e44c7",
            "value": 2
          }
        },
        "5e71c9744afb4f0cafb5fddd8649696c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e159182a27114c2183f1eca35d5266a6",
            "placeholder": "​",
            "style": "IPY_MODEL_4ca8bac370f441178db9a678304055e6",
            "value": " 2/209 [00:12&lt;14:25,  4.18s/it, loss=0.0437, lr=0.00e+00]"
          }
        },
        "a662d32b2f904f9d9b4fe648af2399ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e4ba0f00cd347a18385c0bfba48a20c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36040609f6e54f2788b309dff18a94a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38271774715e4747af6211925791a283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bafa90c3b59458dae6bc01d8d7e44c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e159182a27114c2183f1eca35d5266a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ca8bac370f441178db9a678304055e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "jYJ4ZHDzlk_3",
        "outputId": "1c3ef915-c6f7-4506-b848-b52825325826"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nFull-Scale Text-to-Image Diffusion Pipeline\\nFine-tune Stable Diffusion on custom datasets\\nOptimized for Google Colab (GPU required)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "Full-Scale Text-to-Image Diffusion Pipeline\n",
        "Fine-tune Stable Diffusion on custom datasets\n",
        "Optimized for Google Colab (GPU required)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 1. INSTALL DEPENDENCIES\n",
        "# ============================================================================\n",
        "print(\"Installing dependencies...\")\n",
        "!pip install -q diffusers transformers accelerate datasets torch torchvision\n",
        "!pip install -q xformers  # Memory-efficient attention\n",
        "!pip install -q gradio wandb  # For visualization and logging\n",
        "!pip install -q ftfy regex tqdm  # Text processing\n",
        "!pip install -q bitsandbytes  # For 8-bit optimization\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQdfrTAglpXD",
        "outputId": "00d8a56c-4431-4a37-8fd2-01c1404e6a54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q peft  # For LoRA implementation"
      ],
      "metadata": {
        "id": "rV8irq9cnf0E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 2. IMPORTS\n",
        "# ============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from diffusers import (\n",
        "    StableDiffusionPipeline,\n",
        "    AutoencoderKL,\n",
        "    UNet2DConditionModel,\n",
        "    DDPMScheduler,\n",
        "    DDIMScheduler,\n",
        "    PNDMScheduler,\n",
        "    DPMSolverMultistepScheduler\n",
        ")\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from accelerate import Accelerator\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import gc\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List\n",
        "import wandb\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "rTgiO3Hwl10W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 3. CONFIGURATION\n",
        "# ============================================================================\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    # Model\n",
        "    pretrained_model_name: str = \"runwayml/stable-diffusion-v1-5\"\n",
        "    # Alternative: \"CompVis/stable-diffusion-v1-4\", \"stabilityai/stable-diffusion-2-1\"\n",
        "\n",
        "    # Training\n",
        "    train_batch_size: int = 4\n",
        "    gradient_accumulation_steps: int = 4  # Effective batch size = 4 * 4 = 16\n",
        "    num_epochs: int = 10\n",
        "    learning_rate: float = 5e-6\n",
        "    lr_scheduler: str = \"cosine\"  # \"linear\", \"cosine\", \"constant\"\n",
        "    lr_warmup_steps: int = 500\n",
        "    max_grad_norm: float = 1.0\n",
        "\n",
        "    # Diffusion\n",
        "    num_inference_steps: int = 50\n",
        "    guidance_scale: float = 7.5  # Classifier-free guidance\n",
        "\n",
        "    # Data\n",
        "    dataset_name: str = \"svjack/pokemon-blip-captions-en-zh\"  # Small dataset for demo\n",
        "    # Alternatives: \"nlphuji/flickr30k\", \"ChristophSchuhmann/MS-COCO_2017_URL_TEXT\"\n",
        "    # \"poloclub/diffusiondb\" (2M images), \"Gustavosta/Stable-Diffusion-Prompts\"\n",
        "    image_size: int = 512\n",
        "    center_crop: bool = True\n",
        "    random_flip: bool = True\n",
        "    hf_token: Optional[str] = None  # Set your HuggingFace token if needed\n",
        "\n",
        "    # Optimization\n",
        "    mixed_precision: str = \"no\"  # \"no\", \"fp16\", \"bf16\" - Changed to \"no\" to avoid dtype errors\n",
        "    use_8bit_adam: bool = False  # Requires bitsandbytes\n",
        "    use_ema: bool = True\n",
        "    ema_decay: float = 0.9999\n",
        "\n",
        "    # Advanced\n",
        "    use_lora: bool = True  # Low-Rank Adaptation (memory efficient)\n",
        "    lora_rank: int = 4\n",
        "    noise_offset: float = 0.1  # Improves dark/light generation\n",
        "    prior_preservation: bool = False\n",
        "    prior_loss_weight: float = 1.0\n",
        "\n",
        "    # Validation\n",
        "    validation_prompts: List[str] = None\n",
        "    validation_epochs: int = 2\n",
        "    num_validation_images: int = 4\n",
        "\n",
        "    # Logging\n",
        "    output_dir: str = \"./text2img-model\"\n",
        "    logging_dir: str = \"./logs\"\n",
        "    use_wandb: bool = False\n",
        "    wandb_project: str = \"text2img-diffusion\"\n",
        "    save_model_epochs: int = 5\n",
        "\n",
        "    # Memory optimization\n",
        "    gradient_checkpointing: bool = True\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.validation_prompts is None:\n",
        "            self.validation_prompts = [\n",
        "                \"a photo of a pikachu\",\n",
        "                \"a cute dragon breathing fire\",\n",
        "                \"a futuristic city at sunset\",\n",
        "                \"an astronaut riding a horse\"\n",
        "            ]\n",
        "\n",
        "config = TrainingConfig()\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Model: {config.pretrained_model_name}\")\n",
        "print(f\"  Dataset: {config.dataset_name}\")\n",
        "print(f\"  Batch size: {config.train_batch_size} x {config.gradient_accumulation_steps} = {config.train_batch_size * config.gradient_accumulation_steps}\")\n",
        "print(f\"  Mixed precision: {config.mixed_precision}\")\n",
        "print(f\"  LoRA: {config.use_lora}\")\n",
        "print(f\"  EMA: {config.use_ema}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_p3YGV8maww",
        "outputId": "42e171cd-5f3a-49be-b19f-1152175046f0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  Model: runwayml/stable-diffusion-v1-5\n",
            "  Dataset: svjack/pokemon-blip-captions-en-zh\n",
            "  Batch size: 4 x 4 = 16\n",
            "  Mixed precision: no\n",
            "  LoRA: True\n",
            "  EMA: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 4. INITIALIZE ACCELERATOR (FIXED FOR LORA + FP16)\n",
        "# ============================================================================\n",
        "# CRITICAL: When using LoRA with mixed precision, we need to handle gradient scaling carefully\n",
        "accelerator = Accelerator(\n",
        "    mixed_precision=config.mixed_precision,\n",
        "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "    log_with=\"wandb\" if config.use_wandb else None,\n",
        "    project_dir=config.logging_dir,\n",
        ")\n",
        "\n",
        "os.makedirs(config.output_dir, exist_ok=True)\n",
        "os.makedirs(config.logging_dir, exist_ok=True)\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    if config.use_wandb:\n",
        "        wandb.init(project=config.wandb_project, config=config.__dict__)\n"
      ],
      "metadata": {
        "id": "MVVUVOdzmj6Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 5. LOAD PRETRAINED MODELS\n",
        "# ============================================================================\n",
        "print(\"\\nLoading pretrained models...\")\n",
        "\n",
        "# Load tokenizer and text encoder (CLIP)\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\n",
        "    config.pretrained_model_name,\n",
        "    subfolder=\"tokenizer\"\n",
        ")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\n",
        "    config.pretrained_model_name,\n",
        "    subfolder=\"text_encoder\"\n",
        ")\n",
        "\n",
        "# Load VAE (Variational Autoencoder)\n",
        "vae = AutoencoderKL.from_pretrained(\n",
        "    config.pretrained_model_name,\n",
        "    subfolder=\"vae\"\n",
        ")\n",
        "\n",
        "# Load U-Net\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "    config.pretrained_model_name,\n",
        "    subfolder=\"unet\"\n",
        ")\n",
        "\n",
        "# Load noise scheduler\n",
        "noise_scheduler = DDPMScheduler.from_pretrained(\n",
        "    config.pretrained_model_name,\n",
        "    subfolder=\"scheduler\"\n",
        ")\n",
        "\n",
        "print(f\"✓ Loaded models from {config.pretrained_model_name}\")\n",
        "\n",
        "# Freeze VAE and text encoder (only train U-Net)\n",
        "vae.requires_grad_(False)\n",
        "text_encoder.requires_grad_(False)\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "if config.gradient_checkpointing:\n",
        "    unet.enable_gradient_checkpointing()\n",
        "\n",
        "# Enable xformers memory efficient attention\n",
        "try:\n",
        "    unet.enable_xformers_memory_efficient_attention()\n",
        "    print(\"✓ Enabled xformers memory efficient attention\")\n",
        "except:\n",
        "    print(\"⚠ xformers not available, using default attention\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZS1dinnmk9w",
        "outputId": "12ede512-ae16-406c-aade-574cb347b492"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading pretrained models...\n",
            "✓ Loaded models from runwayml/stable-diffusion-v1-5\n",
            "✓ Enabled xformers memory efficient attention\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 6. LORA IMPLEMENTATION (Optional)\n",
        "# ============================================================================\n",
        "if config.use_lora:\n",
        "    try:\n",
        "        # Try newer API first (diffusers >= 0.20.0)\n",
        "        from diffusers.loaders import AttnProcsLayers\n",
        "        from diffusers.models.attention_processor import LoRAAttnProcessor2_0\n",
        "\n",
        "        lora_attn_procs = {}\n",
        "        for name, attn_processor in unet.attn_processors.items():\n",
        "            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
        "            if name.startswith(\"mid_block\"):\n",
        "                hidden_size = unet.config.block_out_channels[-1]\n",
        "            elif name.startswith(\"up_blocks\"):\n",
        "                block_id = int(name[len(\"up_blocks.\")])\n",
        "                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
        "            elif name.startswith(\"down_blocks\"):\n",
        "                block_id = int(name[len(\"down_blocks.\")])\n",
        "                hidden_size = unet.config.block_out_channels[block_id]\n",
        "\n",
        "            lora_attn_procs[name] = LoRAAttnProcessor2_0(\n",
        "                hidden_size=hidden_size,\n",
        "                cross_attention_dim=cross_attention_dim,\n",
        "                rank=config.lora_rank\n",
        "            )\n",
        "\n",
        "        unet.set_attn_processor(lora_attn_procs)\n",
        "        lora_layers = AttnProcsLayers(unet.attn_processors)\n",
        "\n",
        "    except (ImportError, TypeError):\n",
        "        # Fallback to older API or simpler LoRA implementation\n",
        "        print(\"⚠ Using simplified LoRA implementation\")\n",
        "        from peft import LoraConfig, get_peft_model\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            r=config.lora_rank,\n",
        "            lora_alpha=config.lora_rank,\n",
        "            init_lora_weights=\"gaussian\",\n",
        "            target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
        "        )\n",
        "        unet = get_peft_model(unet, lora_config)\n",
        "        # Explicitly cast LoRA weights to float32 after applying PEFT\n",
        "        for param in unet.parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data = param.data.to(torch.float32)\n",
        "\n",
        "        lora_layers = filter(lambda p: p.requires_grad, unet.parameters())\n",
        "\n",
        "\n",
        "    # Only train LoRA parameters\n",
        "    trainable_params = list(filter(lambda p: p.requires_grad, lora_layers))\n",
        "\n",
        "    print(f\"✓ Enabled LoRA with rank {config.lora_rank}\")\n",
        "    print(f\"  Trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n",
        "else:\n",
        "    trainable_params = unet.parameters()\n",
        "    print(f\"  Training full U-Net: {sum(p.numel() for p in trainable_params):,} parameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO1QN-0Gm97h",
        "outputId": "9b393eae-8b6d-4bb2-be9c-40b0a4b3a9d2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠ Using simplified LoRA implementation\n",
            "✓ Enabled LoRA with rank 4\n",
            "  Trainable parameters: 797,184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 7. EXPONENTIAL MOVING AVERAGE (EMA)\n",
        "# ============================================================================\n",
        "class EMAModel:\n",
        "    def __init__(self, model, decay=0.9999):\n",
        "        self.decay = decay\n",
        "        self.shadow = {}\n",
        "        self.original = {}\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                # Ensure shadow tensors are on the same device as the model parameters\n",
        "                self.shadow[name] = param.data.clone().to(param.data.device)\n",
        "\n",
        "\n",
        "    def update(self, model):\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = self.decay * self.shadow[name] + (1 - self.decay) * param.data\n",
        "\n",
        "    def apply_shadow(self, model):\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.original[name] = param.data.clone()\n",
        "                param.data = self.shadow[name]\n",
        "\n",
        "    def restore(self, model):\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data = self.original[name]\n",
        "\n",
        "ema_model = EMAModel(unet, decay=config.ema_decay) if config.use_ema else None\n",
        "\n",
        "if config.use_ema:\n",
        "    print(f\"✓ Enabled EMA with decay {config.ema_decay}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2-W_Zc0n2Ui",
        "outputId": "c5d6f373-7b2e-4dae-dcf5-eb55453ae662"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Enabled EMA with decay 0.9999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 8. DATASET AND DATALOADER\n",
        "# ============================================================================\n",
        "from PIL import Image # Import Image at the class level\n",
        "\n",
        "class TextImageDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, config):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.config = config\n",
        "\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.Resize(config.image_size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "            transforms.CenterCrop(config.image_size) if config.center_crop else transforms.Lambda(lambda x: x),\n",
        "            transforms.RandomHorizontalFlip() if config.random_flip else transforms.Lambda(lambda x: x),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            item = self.dataset[idx]\n",
        "\n",
        "            # Get image - handle different column names\n",
        "            image = None\n",
        "            for img_col in ['image', 'img', 'picture', 'photo']:\n",
        "                if img_col in item:\n",
        "                    image = item[img_col]\n",
        "                    break\n",
        "\n",
        "            if image is None:\n",
        "                raise ValueError(f\"No image column found in item keys: {item.keys()}\")\n",
        "\n",
        "            # Handle different image formats\n",
        "            if isinstance(image, str):  # URL or file path\n",
        "                try:\n",
        "                    if image.startswith('http'):\n",
        "                        import requests\n",
        "                        from io import BytesIO\n",
        "                        response = requests.get(image, timeout=5)\n",
        "                        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "                    else:\n",
        "                        image = Image.open(image).convert('RGB')\n",
        "                except Exception as e:\n",
        "                    # Return a blank image if download fails\n",
        "                    image = Image.new('RGB', (self.config.image_size, self.config.image_size), (128, 128, 128))\n",
        "            elif not isinstance(image, Image.Image):\n",
        "                 # Convert numpy array or other format\n",
        "                 if hasattr(image, 'shape'):\n",
        "                     image = Image.fromarray(np.array(image)).convert('RGB')\n",
        "                 else:\n",
        "                     image = Image.new('RGB', (self.config.image_size, self.config.image_size), (128, 128, 128))\n",
        "\n",
        "\n",
        "            image = self.transforms(image)\n",
        "\n",
        "            # Get caption - handle different column names\n",
        "            caption = None\n",
        "            for text_col in ['text', 'en_text', 'caption', 'prompt', 'description', 'en_caption']:\n",
        "                if text_col in item:\n",
        "                    caption = item[text_col]\n",
        "                    break\n",
        "\n",
        "            if caption is None:\n",
        "                caption = 'a photo'\n",
        "\n",
        "            if not isinstance(caption, str):\n",
        "                caption = str(caption)\n",
        "\n",
        "            # Tokenize caption\n",
        "            text_inputs = self.tokenizer(\n",
        "                caption,\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.tokenizer.model_max_length,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                \"pixel_values\": image,\n",
        "                \"input_ids\": text_inputs.input_ids[0]\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading item {idx}: {e}\")\n",
        "            # Return a default item\n",
        "            default_image = torch.zeros(3, self.config.image_size, self.config.image_size)\n",
        "            default_text = self.tokenizer(\n",
        "                \"a photo\",\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.tokenizer.model_max_length,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).input_ids[0]\n",
        "            return {\"pixel_values\": default_image, \"input_ids\": default_text}\n",
        "\n",
        "print(\"\\nLoading dataset...\")\n",
        "\n",
        "# Try multiple datasets in order of preference\n",
        "dataset_options = [\n",
        "    (\"svjack/pokemon-blip-captions-en-zh\", None),  # Public Pokemon dataset\n",
        "    (\"poloclub/diffusiondb\", \"2m_random_1k\"),  # 1k subset for quick testing\n",
        "    (\"Gustavosta/Stable-Diffusion-Prompts\", None),  # Text-only prompts\n",
        "]\n",
        "\n",
        "dataset = None\n",
        "for dataset_name, subset in dataset_options:\n",
        "    try:\n",
        "        print(f\"Trying to load: {dataset_name}\")\n",
        "        if subset:\n",
        "            dataset = load_dataset(dataset_name, subset, split=\"train\")\n",
        "        else:\n",
        "            dataset = load_dataset(dataset_name, split=\"train\")\n",
        "        config.dataset_name = dataset_name\n",
        "        print(f\"✓ Successfully loaded {len(dataset)} examples from {dataset_name}\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"  ⚠ Failed to load {dataset_name}: {str(e)[:100]}\")\n",
        "        continue\n",
        "\n",
        "# If all fail, create a simple synthetic dataset\n",
        "if dataset is None:\n",
        "    print(\"\\n⚠ Could not load any dataset. Creating synthetic dataset for testing...\")\n",
        "    from datasets import Dataset as HFDataset\n",
        "    from PIL import Image # Import Image for synthetic data creation\n",
        "\n",
        "    # Create simple colored squares with labels\n",
        "    def create_synthetic_data(num_samples=100):\n",
        "        images = []\n",
        "        texts = []\n",
        "        colors = [\"red\", \"blue\", \"green\", \"yellow\", \"purple\", \"orange\"]\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            # Create a colored square\n",
        "            color_idx = i % len(colors)\n",
        "            color_name = colors[color_idx]\n",
        "\n",
        "            # RGB values for colors\n",
        "            color_map = {\n",
        "                \"red\": (255, 0, 0), \"blue\": (0, 0, 255), \"green\": (0, 255, 0),\n",
        "                \"yellow\": (255, 255, 0), \"purple\": (128, 0, 128), \"orange\": (255, 165, 0)\n",
        "            }\n",
        "\n",
        "            img = Image.new('RGB', (512, 512), color_map[color_name])\n",
        "            images.append(img)\n",
        "            texts.append(f\"a {color_name} square\")\n",
        "\n",
        "        return {\"image\": images, \"text\": texts}\n",
        "\n",
        "    synthetic_data = create_synthetic_data(200)\n",
        "    dataset = HFDataset.from_dict(synthetic_data)\n",
        "    config.dataset_name = \"synthetic_colored_squares\"\n",
        "    print(f\"✓ Created synthetic dataset with {len(dataset)} examples\")\n",
        "\n",
        "# Verify dataset has required columns\n",
        "print(f\"\\nDataset columns: {dataset.column_names}\")\n",
        "\n",
        "# Map columns to standard names if needed\n",
        "needs_rename = False\n",
        "column_mapping = {}\n",
        "\n",
        "if 'image' not in dataset.column_names:\n",
        "    for col in dataset.column_names:\n",
        "        if 'image' in col.lower() or 'img' in col.lower() or 'picture' in col.lower():\n",
        "            column_mapping[col] = 'image'\n",
        "            needs_rename = True\n",
        "            break\n",
        "\n",
        "if 'text' not in dataset.column_names:\n",
        "    for col in dataset.column_names:\n",
        "        if any(keyword in col.lower() for keyword in ['text', 'caption', 'prompt', 'description']):\n",
        "            column_mapping[col] = 'text'\n",
        "            needs_rename = True\n",
        "            break\n",
        "\n",
        "if needs_rename and column_mapping:\n",
        "    print(f\"Renaming columns: {column_mapping}\")\n",
        "    dataset = dataset.rename_columns(column_mapping)\n",
        "    print(f\"✓ New columns: {dataset.column_names}\")\n",
        "elif 'image' not in dataset.column_names or 'text' not in dataset.column_names:\n",
        "    print(f\"⚠ Warning: Expected 'image' and 'text' columns. Available: {dataset.column_names}\")\n",
        "    print(\"⚠ The dataset loader will handle column mapping automatically\")\n",
        "\n",
        "print(f\"\\nDataset info:\")\n",
        "print(f\"  Name: {config.dataset_name}\")\n",
        "print(f\"  Size: {len(dataset)} examples\")\n",
        "print(f\"  Columns: {dataset.column_names}\")\n",
        "if len(dataset) > 0:\n",
        "    # Show sample based on available columns\n",
        "    sample = dataset[0]\n",
        "    text_col = next((col for col in dataset.column_names if any(k in col.lower() for k in ['text', 'caption', 'prompt'])), None)\n",
        "    if text_col:\n",
        "        text_sample = sample[text_col]\n",
        "        if isinstance(text_sample, str):\n",
        "            print(f\"  Sample text: {text_sample[:100]}\")\n",
        "        else:\n",
        "            print(f\"  Sample text: {str(text_sample)[:100]}\")\n",
        "print()\n",
        "\n",
        "train_dataset = TextImageDataset(dataset, tokenizer, config)\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.train_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0  # Set to 0 for Colab\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltnIU3Xkn6Dd",
        "outputId": "513944ec-a67a-4428-dd2c-793ea837ab04"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading dataset...\n",
            "Trying to load: svjack/pokemon-blip-captions-en-zh\n",
            "✓ Successfully loaded 833 examples from svjack/pokemon-blip-captions-en-zh\n",
            "\n",
            "Dataset columns: ['image', 'en_text', 'zh_text']\n",
            "Renaming columns: {'en_text': 'text'}\n",
            "✓ New columns: ['image', 'text', 'zh_text']\n",
            "\n",
            "Dataset info:\n",
            "  Name: svjack/pokemon-blip-captions-en-zh\n",
            "  Size: 833 examples\n",
            "  Columns: ['image', 'text', 'zh_text']\n",
            "  Sample text: a drawing of a green pokemon with red eyes\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 9. OPTIMIZER AND SCHEDULER\n",
        "# ============================================================================\n",
        "if config.use_8bit_adam:\n",
        "    try:\n",
        "        import bitsandbytes as bnb\n",
        "        optimizer = bnb.optim.AdamW8bit(\n",
        "            trainable_params,\n",
        "            lr=config.learning_rate,\n",
        "            betas=(0.9, 0.999),\n",
        "            weight_decay=0.01,\n",
        "            eps=1e-8\n",
        "        )\n",
        "        print(\"✓ Using 8-bit AdamW optimizer\")\n",
        "    except ImportError:\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            trainable_params,\n",
        "            lr=config.learning_rate,\n",
        "            betas=(0.9, 0.999),\n",
        "            weight_decay=0.01,\n",
        "            eps=1e-8\n",
        "        )\n",
        "        print(\"⚠ 8-bit optimizer not available, using standard AdamW\")\n",
        "else:\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        trainable_params,\n",
        "        lr=config.learning_rate,\n",
        "        betas=(0.9, 0.999),\n",
        "        weight_decay=0.01,\n",
        "        eps=1e-8\n",
        "    )\n",
        "\n",
        "# Learning rate scheduler\n",
        "from diffusers.optimization import get_scheduler\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    config.lr_scheduler,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=config.lr_warmup_steps * config.gradient_accumulation_steps,\n",
        "    num_training_steps=len(train_dataloader) * config.num_epochs,\n",
        ")"
      ],
      "metadata": {
        "id": "SBH8dWyBn8_Z"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 10. PREPARE FOR TRAINING WITH ACCELERATOR (FIXED)\n",
        "# ============================================================================\n",
        "# Prepare models - DON'T prepare unet yet if using LoRA, to avoid gradient issues\n",
        "if config.use_lora:\n",
        "    # Only prepare optimizer and dataloader first\n",
        "    optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "        optimizer, train_dataloader, lr_scheduler\n",
        "    )\n",
        "    # Move unet to device manually and keep it unwrapped for LoRA\n",
        "    unet.to(accelerator.device)\n",
        "else:\n",
        "    # Standard preparation for full fine-tuning\n",
        "    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "        unet, optimizer, train_dataloader, lr_scheduler\n",
        "    )\n",
        "\n",
        "vae.to(accelerator.device)\n",
        "text_encoder.to(accelerator.device)\n",
        "weight_dtype = torch.float32\n",
        "if config.mixed_precision == \"fp16\":\n",
        "    weight_dtype = torch.float16\n",
        "elif config.mixed_precision == \"bf16\":\n",
        "    weight_dtype = torch.bfloat16\n",
        "\n",
        "vae.to(dtype=weight_dtype)\n",
        "text_encoder.to(dtype=weight_dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW61coxoa8Wl",
        "outputId": "c1b83343-46c4-4be2-9bee-2f54dd640b49"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CLIPTextModel(\n",
              "  (text_model): CLIPTextTransformer(\n",
              "    (embeddings): CLIPTextEmbeddings(\n",
              "      (token_embedding): Embedding(49408, 768)\n",
              "      (position_embedding): Embedding(77, 768)\n",
              "    )\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 11. TRAINING LOOP (FIXED)\n",
        "# ============================================================================\n",
        "def encode_prompt(batch):\n",
        "    \"\"\"Encode text prompt to embeddings\"\"\"\n",
        "    text_input_ids = batch[\"input_ids\"].to(accelerator.device)\n",
        "    encoder_hidden_states = text_encoder(text_input_ids)[0]\n",
        "    return encoder_hidden_states\n",
        "\n",
        "def train_step(batch, global_step):\n",
        "    \"\"\"Single training step - FIXED for LoRA + FP16\"\"\"\n",
        "    # For LoRA, we need to handle gradients manually without accelerator.accumulate\n",
        "    if config.use_lora:\n",
        "        # Convert images to latent space - ENSURE CORRECT DTYPE\n",
        "        pixel_values = batch[\"pixel_values\"].to(accelerator.device, dtype=weight_dtype)\n",
        "        latents = vae.encode(pixel_values).latent_dist.sample()\n",
        "        latents = latents * vae.config.scaling_factor\n",
        "\n",
        "        # Sample noise\n",
        "        noise = torch.randn_like(latents)\n",
        "\n",
        "        # Add noise offset\n",
        "        if config.noise_offset > 0:\n",
        "            noise += config.noise_offset * torch.randn(\n",
        "                (latents.shape[0], latents.shape[1], 1, 1), device=latents.device\n",
        "            )\n",
        "\n",
        "        # Sample random timestep\n",
        "        bsz = latents.shape[0]\n",
        "        timesteps = torch.randint(\n",
        "            0, noise_scheduler.config.num_train_timesteps, (bsz,),\n",
        "            device=latents.device\n",
        "        ).long()\n",
        "\n",
        "        # Add noise to latents\n",
        "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "        # Get text embeddings\n",
        "        encoder_hidden_states = encode_prompt(batch)\n",
        "\n",
        "        # Predict noise residual\n",
        "        # CRITICAL FIX: Remove explicit .float() calls as mixed_precision is \"no\"\n",
        "        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "        # Compute loss\n",
        "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "            target = noise\n",
        "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "            target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
        "\n",
        "        # Remove explicit .float() from loss calculation\n",
        "        loss = F.mse_loss(model_pred, target, reduction=\"mean\")\n",
        "\n",
        "        # Manual backward without accelerator for LoRA\n",
        "        loss = loss / config.gradient_accumulation_steps\n",
        "        loss.backward()\n",
        "\n",
        "        # Manual gradient step every gradient_accumulation_steps\n",
        "        if (global_step + 1) % config.gradient_accumulation_steps == 0:\n",
        "            # Clip gradients\n",
        "            torch.nn.utils.clip_grad_norm_(trainable_params, config.max_grad_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Update EMA\n",
        "            if config.use_ema:\n",
        "                ema_model.update(unet)\n",
        "\n",
        "        return loss.detach().item() * config.gradient_accumulation_steps\n",
        "\n",
        "    else:\n",
        "        # Standard training step with accelerator for full fine-tuning\n",
        "        with accelerator.accumulate(unet):\n",
        "            # Convert images to latent space - ENSURE CORRECT DTYPE\n",
        "            pixel_values = batch[\"pixel_values\"].to(accelerator.device, dtype=weight_dtype)\n",
        "            latents = vae.encode(pixel_values).latent_dist.sample()\n",
        "            latents = latents * vae.config.scaling_factor\n",
        "\n",
        "            # Sample noise\n",
        "            noise = torch.randn_like(latents)\n",
        "\n",
        "            # Add noise offset\n",
        "            if config.noise_offset > 0:\n",
        "                noise += config.noise_offset * torch.randn(\n",
        "                    (latents.shape[0], latents.shape[1], 1, 1), device=latents.device\n",
        "                )\n",
        "\n",
        "            # Sample random timestep\n",
        "            bsz = latents.shape[0]\n",
        "            timesteps = torch.randint(\n",
        "                0, noise_scheduler.config.num_train_timesteps, (bsz,),\n",
        "                device=latents.device\n",
        "            ).long()\n",
        "\n",
        "            # Add noise to latents\n",
        "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "            # Get text embeddings\n",
        "            encoder_hidden_states = encode_prompt(batch)\n",
        "\n",
        "            # Predict noise residual\n",
        "            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "            # Compute loss\n",
        "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                target = noise\n",
        "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "                target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
        "\n",
        "            # Remove explicit .float() from loss calculation\n",
        "            loss = F.mse_loss(model_pred, target, reduction=\"mean\")\n",
        "\n",
        "            # Backpropagation\n",
        "            accelerator.backward(loss)\n",
        "\n",
        "            if accelerator.sync_gradients:\n",
        "                accelerator.clip_grad_norm_(trainable_params, config.max_grad_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Update EMA\n",
        "            if config.use_ema and accelerator.sync_gradients:\n",
        "                ema_model.update(accelerator.unwrap_model(unet))\n",
        "\n",
        "        return loss.detach().item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(epoch):\n",
        "    \"\"\"Generate validation images - FIXED DTYPE HANDLING\"\"\"\n",
        "    print(f\"\\nGenerating validation images for epoch {epoch}...\")\n",
        "\n",
        "    # Get the correct unet reference\n",
        "    unet_for_inference = unet if config.use_lora else accelerator.unwrap_model(unet)\n",
        "\n",
        "    # Use EMA weights if available\n",
        "    if config.use_ema:\n",
        "        ema_model.apply_shadow(unet_for_inference)\n",
        "\n",
        "    # Create inference pipeline with EXPLICIT dtype handling\n",
        "    pipeline = StableDiffusionPipeline(\n",
        "        vae=vae,\n",
        "        text_encoder=text_encoder,\n",
        "        tokenizer=tokenizer,\n",
        "        unet=unet_for_inference,\n",
        "        scheduler=DPMSolverMultistepScheduler.from_pretrained(\n",
        "            config.pretrained_model_name, subfolder=\"scheduler\"\n",
        "        ),\n",
        "        safety_checker=None,\n",
        "        feature_extractor=None,\n",
        "        requires_safety_checker=False,\n",
        "    )\n",
        "\n",
        "    # CRITICAL FIX: Set pipeline to use the correct dtype\n",
        "    pipeline = pipeline.to(accelerator.device, dtype=weight_dtype)\n",
        "    pipeline.set_progress_bar_config(disable=True)\n",
        "\n",
        "    # Generate images\n",
        "    images = []\n",
        "    for prompt in config.validation_prompts[:config.num_validation_images]:\n",
        "        try:\n",
        "            image = pipeline(\n",
        "                prompt,\n",
        "                num_inference_steps=config.num_inference_steps,\n",
        "                guidance_scale=config.guidance_scale,\n",
        "                generator=torch.Generator(device=accelerator.device).manual_seed(42)\n",
        "            ).images[0]\n",
        "            images.append(image)\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating image for '{prompt}': {e}\")\n",
        "            # Create a placeholder image\n",
        "            images.append(Image.new('RGB', (512, 512), (128, 128, 128)))\n",
        "\n",
        "    # Display images\n",
        "    fig, axes = plt.subplots(1, len(images), figsize=(5*len(images), 5))\n",
        "    if len(images) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for idx, (img, prompt) in enumerate(zip(images, config.validation_prompts[:len(images)])):\n",
        "        axes[idx].imshow(img)\n",
        "        axes[idx].set_title(prompt, fontsize=10, wrap=True)\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{config.output_dir}/validation_epoch_{epoch}.png\", dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Log to wandb\n",
        "    if config.use_wandb and accelerator.is_main_process:\n",
        "        wandb.log({\n",
        "            \"validation\": [wandb.Image(img, caption=prompt)\n",
        "                          for img, prompt in zip(images, config.validation_prompts[:len(images)])]\n",
        "        }, step=epoch)\n",
        "\n",
        "    # Restore original weights\n",
        "    if config.use_ema:\n",
        "        ema_model.restore(unet_for_inference)\n",
        "\n",
        "    del pipeline\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "global_step = 0\n",
        "for epoch in range(config.num_epochs):\n",
        "    unet.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(\n",
        "        train_dataloader,\n",
        "        desc=f\"Epoch {epoch+1}/{config.num_epochs}\",\n",
        "        disable=not accelerator.is_local_main_process\n",
        "    )\n",
        "\n",
        "    for step, batch in enumerate(progress_bar):\n",
        "        loss = train_step(batch, global_step)\n",
        "        epoch_loss += loss\n",
        "\n",
        "        # Update global step based on whether we're using manual accumulation (LoRA) or accelerator\n",
        "        if config.use_lora:\n",
        "            if (global_step + 1) % config.gradient_accumulation_steps == 0:\n",
        "                global_step += 1\n",
        "        else:\n",
        "            if accelerator.sync_gradients:\n",
        "                global_step += 1\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f'{loss:.4f}',\n",
        "            'lr': f'{lr_scheduler.get_last_lr()[0]:.2e}'\n",
        "        })\n",
        "\n",
        "        # Log to wandb\n",
        "        if config.use_wandb and accelerator.is_main_process and global_step % 100 == 0:\n",
        "            wandb.log({\n",
        "                \"train_loss\": loss,\n",
        "                \"learning_rate\": lr_scheduler.get_last_lr()[0],\n",
        "                \"epoch\": epoch\n",
        "            }, step=global_step)\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_dataloader)\n",
        "    print(f\"\\nEpoch {epoch+1} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    if (epoch + 1) % config.validation_epochs == 0:\n",
        "        if accelerator.is_main_process:\n",
        "            try:\n",
        "                validate(epoch + 1)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ Validation failed: {e}\")\n",
        "                print(\"Continuing training...\")\n",
        "\n",
        "    # Save model\n",
        "    if (epoch + 1) % config.save_model_epochs == 0:\n",
        "        if accelerator.is_main_process:\n",
        "            print(f\"\\nSaving model checkpoint at epoch {epoch+1}...\")\n",
        "\n",
        "            try:\n",
        "                if config.use_lora:\n",
        "                    # For LoRA, save the attention processors\n",
        "                    unet.save_attn_procs(f\"{config.output_dir}/lora_weights_epoch_{epoch+1}.pt\")\n",
        "                    print(f\"✓ Saved LoRA weights\")\n",
        "                else:\n",
        "                    accelerator.unwrap_model(unet).save_pretrained(f\"{config.output_dir}/unet_epoch_{epoch+1}\")\n",
        "                    print(f\"✓ Saved U-Net weights\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ Failed to save checkpoint: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545,
          "referenced_widgets": [
            "828c6c06b35e438f8715d5e7daa1d5a1",
            "e9984bf2b2564a5cbcc2852bcc77a403",
            "651163a3e8e4444da63183877e7d42b2",
            "2168f7c1c5b544fca0fa20822241f617",
            "14a9545524304b78ab7d086ddadaedbc",
            "faefabf89cf547cdbd0d044543fd59d7",
            "d7dd82379abd45839ea27332432b6369",
            "1b53acde324143b4ac01e4140e4d9887",
            "6df901b7cbf04d039176bff474bc1a0a",
            "6db269e192d249a5a175f79b4a7b45d1",
            "6993a97f02bb4bd4babf9e7401c471b7",
            "5800b2b9957d488ba8a89c7760e14433",
            "2b80a22ac036460f9c320ba5dbeaeeff",
            "d8eac01d479e4e689a19e8a1f3d4b67c",
            "e955cb6b90dd4e3fbcc24eeea0717a33",
            "3e8d8bbc75f243cd9ca08320b00c05cf",
            "b3e6c3b0d26b4e49a6171a77faadff82",
            "49dee07e1c5b45cc8d664166f1fa263c",
            "d6917c1bc6aa4cc0a81a3833eb120933",
            "5f0cac18e7b04d3da76ae10e03715772",
            "5d7ac9ffa2b549d98f1cf1da41600942",
            "75582ded990946849816ed84c24d1846",
            "0322a4b9ec904944b23c29b9b025346a",
            "f02ae30a87bf4c6b9f9d6e53e8eb0cb4",
            "5bd1eb08c1a74847a77b42da489893da",
            "43d894059f4f478a92102cfbc73d6ebc",
            "8ca0626871ad435893db951c15acee9d",
            "8e985e83fedb47f2af7b9bc3bedd2350",
            "989495a6d4004616abb6c0704ed7a983",
            "0508799203d54ffeac07c8b1a57e603a",
            "2c50e777888049879c8a13317f06ff1f",
            "18aa52964d75451bad512d51afab2d38",
            "ee5f24da197647c49272f0bc18e54b82",
            "9eac30a469c041ca9cbda5d3be0901fb",
            "6445b742360e44b4888c645ced902591",
            "fd5d1da8295b495c93c439d07451ee8d",
            "d9a38eaa8d1448b2ae0e8447b855de47",
            "cdb329d5ec684f7eb2a674af1f7bc471",
            "ba2f59f2cf544edea12da493d5194c77",
            "0641c24dc1384bddb97aa1f781d5b438",
            "8a6645b526fd434187d5e05d066919e5",
            "3593d416275f4f34af138e1a67125931",
            "5f5829be730c42c084fe126b3fe944fc",
            "39807bd944594c3a86bcecb02e436a44"
          ]
        },
        "id": "q78jyADdkzBI",
        "outputId": "58f526fb-c845-48ec-d52c-3b9e1f766f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "STARTING TRAINING\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/10:   0%|          | 0/209 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "828c6c06b35e438f8715d5e7daa1d5a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 - Average Loss: 0.0642\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 2/10:   0%|          | 0/209 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5800b2b9957d488ba8a89c7760e14433"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 - Average Loss: 0.0624\n",
            "\n",
            "Generating validation images for epoch 2...\n",
            "⚠ Validation failed: 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight'\n",
            "Continuing training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 3/10:   0%|          | 0/209 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0322a4b9ec904944b23c29b9b025346a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3 - Average Loss: 0.0598\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 4/10:   0%|          | 0/209 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9eac30a469c041ca9cbda5d3be0901fb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 11. TRAINING LOOP\n",
        "# ============================================================================\n",
        "def train_step(batch, global_step):\n",
        "    \"\"\"Single training step\"\"\"\n",
        "    with accelerator.accumulate(unet):\n",
        "        # Convert images to latent space\n",
        "        pixel_values = batch[\"pixel_values\"].to(accelerator.device, dtype=weight_dtype)\n",
        "        latents = vae.encode(pixel_values).latent_dist.sample()\n",
        "        latents = latents * vae.config.scaling_factor\n",
        "\n",
        "        # Sample noise\n",
        "        noise = torch.randn_like(latents)\n",
        "\n",
        "        # Add noise offset for better dark/light generation\n",
        "        if config.noise_offset > 0:\n",
        "            noise += config.noise_offset * torch.randn(\n",
        "                (latents.shape[0], latents.shape[1], 1, 1), device=latents.device\n",
        "            )\n",
        "\n",
        "        # Sample random timestep\n",
        "        bsz = latents.shape[0]\n",
        "        timesteps = torch.randint(\n",
        "            0, noise_scheduler.config.num_train_timesteps, (bsz,),\n",
        "            device=latents.device\n",
        "        ).long()\n",
        "\n",
        "        # Add noise to latents (forward diffusion)\n",
        "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "        # Get text embeddings\n",
        "        text_input_ids = batch[\"input_ids\"].to(accelerator.device)\n",
        "        encoder_hidden_states = text_encoder(text_input_ids, return_dict=False)[0]\n",
        "\n",
        "        # Convert encoder hidden states to correct dtype\n",
        "        encoder_hidden_states = encoder_hidden_states.to(dtype=weight_dtype)\n",
        "\n",
        "        # Predict noise residual\n",
        "        model_pred = unet(\n",
        "            noisy_latents.to(dtype=weight_dtype),\n",
        "            timesteps,\n",
        "            encoder_hidden_states,\n",
        "            return_dict=False\n",
        "        )[0]\n",
        "\n",
        "        # Compute loss\n",
        "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "            target = noise\n",
        "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "            target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
        "\n",
        "        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
        "\n",
        "        # Backpropagation\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        if accelerator.sync_gradients:\n",
        "            params_to_clip = trainable_params\n",
        "            accelerator.clip_grad_norm_(params_to_clip, config.max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Update EMA\n",
        "        if config.use_ema and accelerator.sync_gradients:\n",
        "            ema_model.update(accelerator.unwrap_model(unet))\n",
        "\n",
        "    return loss.detach().item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(epoch):\n",
        "    \"\"\"Generate validation images - FIXED for LoRA\"\"\"\n",
        "    print(f\"\\nGenerating validation images for epoch {epoch}...\")\n",
        "\n",
        "    try:\n",
        "        if config.use_lora:\n",
        "            # For LoRA: Create a fresh pipeline and load LoRA weights\n",
        "            print(\"Setting up inference pipeline with LoRA...\")\n",
        "\n",
        "            # Create base pipeline\n",
        "            pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "                config.pretrained_model_name,\n",
        "                torch_dtype=weight_dtype,\n",
        "                safety_checker=None,\n",
        "                requires_safety_checker=False,\n",
        "            )\n",
        "\n",
        "            # Save current LoRA weights temporarily\n",
        "            temp_lora_path = f\"{config.output_dir}/temp_lora_validation.pt\"\n",
        "            unet.save_attn_procs(temp_lora_path)\n",
        "\n",
        "            # Load LoRA weights into the pipeline's unet\n",
        "            pipeline.unet.load_attn_procs(temp_lora_path)\n",
        "\n",
        "            # Clean up temp file\n",
        "            import os\n",
        "            if os.path.exists(temp_lora_path):\n",
        "                os.remove(temp_lora_path)\n",
        "\n",
        "        else:\n",
        "            # For full fine-tuning: use the unwrapped model\n",
        "            unet_for_inference = accelerator.unwrap_model(unet)\n",
        "\n",
        "            # Use EMA weights if available\n",
        "            if config.use_ema:\n",
        "                ema_model.apply_shadow(unet_for_inference)\n",
        "\n",
        "            pipeline = StableDiffusionPipeline(\n",
        "                vae=vae,\n",
        "                text_encoder=text_encoder,\n",
        "                tokenizer=tokenizer,\n",
        "                unet=unet_for_inference,\n",
        "                scheduler=DPMSolverMultistepScheduler.from_pretrained(\n",
        "                    config.pretrained_model_name, subfolder=\"scheduler\"\n",
        "                ),\n",
        "                safety_checker=None,\n",
        "                feature_extractor=None,\n",
        "                requires_safety_checker=False,\n",
        "            )\n",
        "\n",
        "        pipeline = pipeline.to(accelerator.device)\n",
        "        pipeline.set_progress_bar_config(disable=True)\n",
        "\n",
        "        # Enable memory optimizations\n",
        "        if hasattr(pipeline, 'enable_attention_slicing'):\n",
        "            pipeline.enable_attention_slicing()\n",
        "        if hasattr(pipeline, 'enable_vae_slicing'):\n",
        "            pipeline.enable_vae_slicing()\n",
        "\n",
        "        # Generate images\n",
        "        images = []\n",
        "        for prompt in config.validation_prompts[:config.num_validation_images]:\n",
        "            try:\n",
        "                image = pipeline(\n",
        "                    prompt,\n",
        "                    num_inference_steps=config.num_inference_steps,\n",
        "                    guidance_scale=config.guidance_scale,\n",
        "                    generator=torch.Generator(device=accelerator.device).manual_seed(42)\n",
        "                ).images[0]\n",
        "                images.append(image)\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠ Error generating '{prompt}': {str(e)[:100]}\")\n",
        "                # Create a placeholder\n",
        "                images.append(Image.new('RGB', (512, 512), (128, 128, 128)))\n",
        "\n",
        "        # Display images\n",
        "        if len(images) > 0:\n",
        "            fig, axes = plt.subplots(1, len(images), figsize=(5*len(images), 5))\n",
        "            if len(images) == 1:\n",
        "                axes = [axes]\n",
        "\n",
        "            for idx, (img, prompt) in enumerate(zip(images, config.validation_prompts[:len(images)])):\n",
        "                axes[idx].imshow(img)\n",
        "                axes[idx].set_title(prompt, fontsize=10, wrap=True)\n",
        "                axes[idx].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{config.output_dir}/validation_epoch_{epoch}.png\", dpi=150, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "            # Log to wandb\n",
        "            if config.use_wandb and accelerator.is_main_process:\n",
        "                wandb.log({\n",
        "                    \"validation\": [wandb.Image(img, caption=prompt)\n",
        "                                  for img, prompt in zip(images, config.validation_prompts[:len(images)])]\n",
        "                }, step=epoch)\n",
        "\n",
        "        # Restore original weights for non-LoRA\n",
        "        if not config.use_lora and config.use_ema:\n",
        "            ema_model.restore(accelerator.unwrap_model(unet))\n",
        "\n",
        "        # Cleanup\n",
        "        del pipeline\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        print(\"✓ Validation complete\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Validation failed with error: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(\"Continuing training...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "global_step = 0\n",
        "for epoch in range(config.num_epochs):\n",
        "    unet.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(\n",
        "        train_dataloader,\n",
        "        desc=f\"Epoch {epoch+1}/{config.num_epochs}\",\n",
        "        disable=not accelerator.is_local_main_process\n",
        "    )\n",
        "\n",
        "    for step, batch in enumerate(progress_bar):\n",
        "        loss = train_step(batch, global_step)\n",
        "        epoch_loss += loss\n",
        "\n",
        "        if accelerator.sync_gradients:\n",
        "            global_step += 1\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f'{loss:.4f}',\n",
        "            'lr': f'{lr_scheduler.get_last_lr()[0]:.2e}'\n",
        "        })\n",
        "\n",
        "        # Log to wandb\n",
        "        if config.use_wandb and accelerator.is_main_process and global_step % 100 == 0:\n",
        "            wandb.log({\n",
        "                \"train_loss\": loss,\n",
        "                \"learning_rate\": lr_scheduler.get_last_lr()[0],\n",
        "                \"epoch\": epoch\n",
        "            }, step=global_step)\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_dataloader)\n",
        "    print(f\"\\nEpoch {epoch+1} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    if (epoch + 1) % config.validation_epochs == 0:\n",
        "        if accelerator.is_main_process:\n",
        "            validate(epoch + 1)\n",
        "\n",
        "    # Save model\n",
        "    if (epoch + 1) % config.save_model_epochs == 0:\n",
        "        if accelerator.is_main_process:\n",
        "            print(f\"\\nSaving model checkpoint at epoch {epoch+1}...\")\n",
        "\n",
        "            # Save U-Net weights\n",
        "            if config.use_lora:\n",
        "                unet_lora = accelerator.unwrap_model(unet)\n",
        "                unet_lora.save_attn_procs(f\"{config.output_dir}/lora_weights_epoch_{epoch+1}.pt\")\n",
        "                print(f\"✓ Saved LoRA weights\")\n",
        "            else:\n",
        "                accelerator.unwrap_model(unet).save_pretrained(f\"{config.output_dir}/unet_epoch_{epoch+1}\")\n",
        "                print(f\"✓ Saved U-Net weights\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542,
          "referenced_widgets": [
            "754de80131de42a7a6d6cf6c34e2de9d",
            "abc70629c2ac407882ad3a754c2cc97d",
            "4e7a8aeef944416ab7feecb5aab378e5",
            "5e71c9744afb4f0cafb5fddd8649696c",
            "a662d32b2f904f9d9b4fe648af2399ba",
            "8e4ba0f00cd347a18385c0bfba48a20c",
            "36040609f6e54f2788b309dff18a94a0",
            "38271774715e4747af6211925791a283",
            "9bafa90c3b59458dae6bc01d8d7e44c7",
            "e159182a27114c2183f1eca35d5266a6",
            "4ca8bac370f441178db9a678304055e6"
          ]
        },
        "id": "TLqmbhJOByBf",
        "outputId": "646034f3-514b-4c79-e679-c4dae7317092"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "STARTING TRAINING\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/10:   0%|          | 0/209 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "754de80131de42a7a6d6cf6c34e2de9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2047898123.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2047898123.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(batch, global_step)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Update EMA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_ema\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_gradients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mema_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3646008175.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshadow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshadow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_shadow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'base_model.model.base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.default.weight'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 12. FINAL MODEL SAVE\n",
        "# ============================================================================\n",
        "if accelerator.is_main_process:\n",
        "    print(\"\\nSaving final model...\")\n",
        "\n",
        "    if config.use_ema:\n",
        "        ema_model.apply_shadow(accelerator.unwrap_model(unet))\n",
        "\n",
        "    if config.use_lora:\n",
        "        accelerator.unwrap_model(unet).save_attn_procs(f\"{config.output_dir}/lora_weights_final.pt\")\n",
        "    else:\n",
        "        accelerator.unwrap_model(unet).save_pretrained(f\"{config.output_dir}/unet_final\")\n",
        "\n",
        "    print(f\"✓ Model saved to {config.output_dir}\")\n"
      ],
      "metadata": {
        "id": "CJcFzQ9lbFVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 13. INFERENCE PIPELINE\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING INFERENCE PIPELINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load the trained model\n",
        "if config.use_ema:\n",
        "    ema_model.apply_shadow(accelerator.unwrap_model(unet))\n",
        "\n",
        "inference_pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "    config.pretrained_model_name,\n",
        "    unet=accelerator.unwrap_model(unet),\n",
        "    torch_dtype=weight_dtype,\n",
        "    safety_checker=None,\n",
        "    requires_safety_checker=False,\n",
        ")\n",
        "inference_pipeline.scheduler = DPMSolverMultistepScheduler.from_pretrained(\n",
        "    config.pretrained_model_name, subfolder=\"scheduler\"\n",
        ")\n",
        "inference_pipeline.to(accelerator.device)\n",
        "\n",
        "print(\"✓ Inference pipeline ready!\")"
      ],
      "metadata": {
        "id": "8areBzWSbSlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 14. GENERATE SAMPLE IMAGES\n",
        "# ============================================================================\n",
        "def generate_image(prompt, num_images=1, seed=None):\n",
        "    \"\"\"Generate images from text prompt\"\"\"\n",
        "    if seed is not None:\n",
        "        generator = torch.Generator(device=accelerator.device).manual_seed(seed)\n",
        "    else:\n",
        "        generator = None\n",
        "\n",
        "    images = inference_pipeline(\n",
        "        prompt,\n",
        "        num_images_per_prompt=num_images,\n",
        "        num_inference_steps=config.num_inference_steps,\n",
        "        guidance_scale=config.guidance_scale,\n",
        "        generator=generator\n",
        "    ).images\n",
        "\n",
        "    return images\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GENERATING SAMPLE IMAGES\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "sample_prompts = [\n",
        "    \"a beautiful landscape with mountains and lake\",\n",
        "    \"a cute robot playing guitar\",\n",
        "    \"a magical forest with glowing mushrooms\",\n",
        "    \"a cyberpunk city at night\"\n",
        "]\n",
        "\n",
        "for prompt in sample_prompts:\n",
        "    print(f\"Generating: '{prompt}'\")\n",
        "    images = generate_image(prompt, num_images=1, seed=42)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(images[0])\n",
        "    plt.title(prompt, fontsize=12, wrap=True)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4VDViHGVbWGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 15. GRADIO INTERFACE (OPTIONAL)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING GRADIO INTERFACE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def generate_gradio(prompt, num_steps, guidance, seed):\n",
        "    \"\"\"Gradio generation function\"\"\"\n",
        "    if seed == -1:\n",
        "        seed = None\n",
        "\n",
        "    temp_pipeline = inference_pipeline\n",
        "    temp_pipeline.scheduler.config.num_train_timesteps = num_steps\n",
        "\n",
        "    image = temp_pipeline(\n",
        "        prompt,\n",
        "        num_inference_steps=num_steps,\n",
        "        guidance_scale=guidance,\n",
        "        generator=torch.Generator(device=accelerator.device).manual_seed(seed) if seed else None\n",
        "    ).images[0]\n",
        "\n",
        "    return image\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=generate_gradio,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Prompt\", placeholder=\"Enter your text prompt here...\"),\n",
        "        gr.Slider(10, 100, value=50, step=5, label=\"Inference Steps\"),\n",
        "        gr.Slider(1, 20, value=7.5, step=0.5, label=\"Guidance Scale\"),\n",
        "        gr.Number(value=42, label=\"Seed (-1 for random)\")\n",
        "    ],\n",
        "    outputs=gr.Image(label=\"Generated Image\", type=\"pil\"),\n",
        "    title=\"Text-to-Image Diffusion Model\",\n",
        "    description=\"Generate images from text descriptions using your fine-tuned model!\",\n",
        "    examples=[\n",
        "        [\"a photo of a cute puppy\", 50, 7.5, 42],\n",
        "        [\"a futuristic cityscape\", 50, 7.5, 123],\n",
        "        [\"an astronaut riding a horse on mars\", 50, 7.5, 456],\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Gradio interface created!\")\n",
        "print(\"\\nLaunch with: demo.launch(share=True)\")\n"
      ],
      "metadata": {
        "id": "Rm29EkFobau0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 16. USAGE INSTRUCTIONS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"USAGE INSTRUCTIONS\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "# Generate single image:\n",
        "images = generate_image(\"a beautiful sunset over ocean\", num_images=1, seed=42)\n",
        "\n",
        "# Generate multiple images:\n",
        "images = generate_image(\"a cute cat\", num_images=4, seed=123)\n",
        "\n",
        "# Launch Gradio interface:\n",
        "demo.launch(share=True)\n",
        "\n",
        "# Load model later:\n",
        "from diffusers import StableDiffusionPipeline\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    unet=UNet2DConditionModel.from_pretrained(\"./text2img-model/unet_final\")\n",
        ")\n",
        "\n",
        "# For LoRA weights:\n",
        "pipeline.unet.load_attn_procs(\"./text2img-model/lora_weights_final.pt\")\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n✓ Setup complete! Your text-to-image model is ready to use.\")"
      ],
      "metadata": {
        "id": "C82vf4S1bdUM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}